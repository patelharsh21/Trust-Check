{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feature_extractor as fe\n",
    "import content_features as ctnfe\n",
    "import url_features as urlfe\n",
    "import tldextract\n",
    "import external_features as trdfe\n",
    "import re\n",
    "import whois    \n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process and update csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_csv(input_file, output_file):\n",
    "#     # Open the input CSV file for reading\n",
    "#     with open(input_file, 'r', newline='') as infile:\n",
    "#         reader = csv.reader(infile)\n",
    "#         rows = list(reader)  # Read all rows from the CSV\n",
    "\n",
    "#         # Determine the header and data rows\n",
    "#         header = rows[0]  # Assuming the first row is header\n",
    "#         data_rows = rows[1:]  # Following rows are data\n",
    "\n",
    "#         # Process each row in the CSV\n",
    "#         for i, row in enumerate(data_rows):\n",
    "#             url = row[0]  # Assuming URL is in the first column\n",
    "#             extracted_values = fe.extract_features(url)\n",
    "\n",
    "#             if extracted_values is not None:\n",
    "#                 # Update the row with extracted values (excluding the first column and last column \"status\")\n",
    "#                 updated_row = [url] + extracted_values + [row[-1]]\n",
    "#                 # Replace the old row with updated row in the data_rows list\n",
    "#                 data_rows[i] = updated_row\n",
    "#                 print(f\"Row {i} updated\")\n",
    "#             else:\n",
    "#                 print(f\"Skipping row {i+2} (URL: {url}) because feature extraction returned None.\")\n",
    "\n",
    "#     # Write the updated data back to the output CSV file\n",
    "#     with open(output_file, 'w', newline='') as outfile:\n",
    "#         writer = csv.writer(outfile)\n",
    "#         writer.writerow(header)  # Write the header\n",
    "#         writer.writerows(data_rows)  # Write the updated data rows\n",
    "\n",
    "#     print(f\"CSV file '{output_file}' has been updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_csv_file = '/home/sahil/Desktop/phishing_detection/dataset.csv'  # Replace with your input CSV file path\n",
    "#     output_csv_file = '/home/sahil/Desktop/phishing_detection/dupli_dataset.csv'  # Replace with your desired output CSV file path\n",
    "#     process_csv(input_csv_file, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_csv(input_file, output_file):\n",
    "#     # Open the input CSV file for reading and output CSV file for writing\n",
    "#     with open(input_file, 'r', newline='') as infile, open(output_file, 'w', newline='') as outfile:\n",
    "#         reader = csv.DictReader(infile)\n",
    "#         writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n",
    "#         writer.writeheader()\n",
    "\n",
    "#         # Process each row in the CSV\n",
    "#         for i, row in enumerate(reader):\n",
    "#             url = row['url']  # Assuming 'url' is the header for the URL column\n",
    "#             extracted_values = fe.extract_features(url)\n",
    "#             status = row['status']\n",
    "\n",
    "#             if extracted_values is not None:\n",
    "#                 # Update the row with extracted values (excluding the 'status' column)\n",
    "#                 row.update(extracted_values)\n",
    "#                 row['status'] = status  # Ensure 'status' column remains unchanged\n",
    "#                 writer.writerow(row)\n",
    "#                 print(f\"Row {i+2} updated\")\n",
    "#             else:\n",
    "#                 print(f\"Skipping row {i+2} (URL: {url}) because feature extraction returned None.\")\n",
    "\n",
    "#     print(f\"CSV file '{output_file}' has been updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_csv(input_file, output_file):\n",
    "#     # Open the input CSV file for reading\n",
    "#     with open(input_file, 'r', newline='') as infile, open(output_file, 'w', newline='') as outfile:\n",
    "#         reader = csv.reader(infile)\n",
    "#         writer = csv.writer(outfile)\n",
    "#         rows = list(reader)  # Read all rows from the CSV\n",
    "\n",
    "#         # Determine the header and data rows\n",
    "#         header = rows[0]  # Assuming the first row is header\n",
    "#         data_rows = rows[1:]  # Following rows are data\n",
    "\n",
    "#         writer.writerow(header)  # Write the header\n",
    "\n",
    "#         # Process each row in the CSV\n",
    "#         for i, row in enumerate(data_rows):\n",
    "#             url = row[0]  # Assuming URL is in the first column\n",
    "#             extracted_values = fe.extract_features(url)\n",
    "\n",
    "#             if extracted_values is not None:\n",
    "#                 # Update the row with extracted values (excluding the first column and last column \"status\")\n",
    "#                 updated_row = [url] + extracted_values + [row[-1]]\n",
    "#                 # Replace the old row with updated row in the data_rows list\n",
    "#                 data_rows[i] = updated_row\n",
    "#                 writer.writerow(updated_row)\n",
    "#                 print(f\"Row {i} updated\")\n",
    "#             else:\n",
    "#                 print(f\"Skipping row {i+2} (URL: {url}) because feature extraction returned None.\")\n",
    "\n",
    "\n",
    "\n",
    "#     print(f\"CSV file '{output_file}' has been updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.reddit.com/r/AmazonFC/comments/1e8jvec/someone_cant_handle_met/', 75, 14, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 8, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0.02666666666666667, 0.0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 1, 0, 10, 5, 1, 3, 1, 8, 6, 8, 5.3, 4.5, 5.5, 0, 1, 0, 0, 0, 0, 76, 0.18421052631578946, 0.8157894736842105, 0.0, 1, 0.0, 0.03225806451612903, 0.0, 0.0967741935483871, 0, 1, 5.88235294117647, 0, 0.0, 100.0, 0, 0, 0, 20.0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.reddit.com/r/AmazonFC/comments/1e8jvec/someone_cant_handle_met/\"\n",
    "print(fe.extract_features(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write the extracted features into csv file(multiprocessing not applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(input_file, output_file):\n",
    "    # Open the input CSV file for reading\n",
    "    with open(input_file, 'r', newline='') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        rows = list(reader)  # Read all rows from the CSV\n",
    "\n",
    "        # Determine the header and data rows\n",
    "        header = rows[0]  # Assuming the first row is header\n",
    "        data_rows = rows[1:100]  # Following rows are data\n",
    "\n",
    "    # Open the output CSV file for appending\n",
    "    with open(output_file, 'a', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        # Process each row in the CSV\n",
    "        for i, row in enumerate(data_rows):\n",
    "            url = row[0]  # Assuming URL is in the first column\n",
    "            extracted_values = fe.extract_features(url)\n",
    "\n",
    "            if extracted_values is not None:\n",
    "                # Ensure extracted_values is a list or tuple\n",
    "                if not isinstance(extracted_values, (list, tuple)):\n",
    "                    print(f\"Skipping row {i+1} (URL: {url}) - Extracted values not a list/tuple.\")\n",
    "                    continue  # Skip to the next row\n",
    "\n",
    "                # Update the row with extracted values (excluding the first column and last column \"status\")\n",
    "                updated_row = extracted_values + [row[-1]] # Directly use extracted values since it already includes the URL and status\n",
    "\n",
    "                # Append the updated row to the output CSV file\n",
    "                writer.writerow(updated_row)\n",
    "                print(f\"Row {i+1} updated\")\n",
    "            else:\n",
    "                print(f\"Skipping row {i+1} (URL: {url}) because feature extraction returned None.\")\n",
    "\n",
    "    print(f\"CSV file '{output_file}' has been updated.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_file = '/home/sahil/Desktop/phishing_detection/dataset.csv'  # Replace with your input CSV file path\n",
    "    output_csv_file = '/home/sahil/Desktop/phishing_detection/data3.csv'  # Replace with your desired output CSV file path\n",
    "    process_csv(input_csv_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for multiprocessing (order of results is MAINTAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import multiprocessing as mp\n",
    "\n",
    "def process_url(index_row_tuple):\n",
    "    index, row = index_row_tuple\n",
    "    url = row[0]  # Assuming URL is in the first column\n",
    "    extracted_values = fe.extract_features(url)\n",
    "    if extracted_values is not None:\n",
    "        # Ensure extracted_values is a list or tuple\n",
    "        if not isinstance(extracted_values, (list, tuple)):\n",
    "            print(f\"Skipping row {index+1} with URL: {url} - Extracted values not a list/tuple.\")\n",
    "            return index, None  # Return index with None\n",
    "\n",
    "        # Update the row with extracted values (excluding the first column and last column \"status\")\n",
    "        updated_row = extracted_values + [row[-1]]  # Directly use extracted values since it already includes the URL and status\n",
    "        return index, updated_row\n",
    "    else:\n",
    "        print(f\"Skipping row {index+1} with URL: {url} because feature extraction returned None.\")\n",
    "        return index, None\n",
    "\n",
    "def process_csv(input_file, output_file, num_processes=4):\n",
    "    # Open the input CSV file for reading\n",
    "    with open(input_file, 'r', newline='') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        rows = list(reader)  # Read all rows from the CSV\n",
    "\n",
    "        # Determine the header and data rows\n",
    "        header = rows[0]  # Assuming the first row is header\n",
    "        data_rows = rows[1:5]  # Following rows are data\n",
    "\n",
    "    # Create a pool of worker processes\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        # Process each row in parallel, preserving the order\n",
    "        results = pool.imap(process_url, enumerate(data_rows))\n",
    "\n",
    "    # Collect results and maintain order\n",
    "    ordered_results = sorted(results, key=lambda x: x[0])\n",
    "    filtered_results = [row for index, row in ordered_results if row is not None]\n",
    "\n",
    "    # Open the output CSV file for appending\n",
    "    with open(output_file, 'a', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerows(filtered_results)\n",
    "        print(f\"CSV file '{output_file}' has been updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/phishing/lib/python3.12/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m input_csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/sahil/Desktop/phishing_detection/dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your input CSV file path\u001b[39;00m\n\u001b[1;32m      4\u001b[0m output_csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/sahil/Desktop/phishing_detection/data4.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your desired output CSV file path\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mprocess_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_csv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m, in \u001b[0;36mprocess_csv\u001b[0;34m(input_file, output_file, num_processes)\u001b[0m\n\u001b[1;32m     34\u001b[0m     results \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mimap(process_url, \u001b[38;5;28menumerate\u001b[39m(data_rows))\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Collect results and maintain order\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m ordered_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m filtered_results \u001b[38;5;241m=\u001b[39m [row \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m ordered_results \u001b[38;5;28;01mif\u001b[39;00m row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Open the output CSV file for appending\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/phishing/lib/python3.12/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m~/anaconda3/envs/phishing/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_file = '/home/sahil/Desktop/phishing_detection/dataset.csv'  # Replace with your input CSV file path\n",
    "    output_csv_file = '/home/sahil/Desktop/phishing_detection/data4.csv'  # Replace with your desired output CSV file path\n",
    "    process_csv(input_csv_file, output_csv_file, num_processes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for multiprocessing (order of results is not maintained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file '/home/sahil/Desktop/phishing_detection/top_200_extracted.csv' has been updated.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "def process_url(row):\n",
    "    url = row[0]  # Assuming URL is in the first column\n",
    "    extracted_values = fe.extract_features(url)\n",
    "    if extracted_values is not None:\n",
    "        # Ensure extracted_values is a list or tuple\n",
    "        if not isinstance(extracted_values, (list, tuple)):\n",
    "            print(f\"Skipping row with URL: {url} - Extracted values not a list/tuple.\")\n",
    "            return None  # Skip to the next row\n",
    "\n",
    "        # Update the row with extracted values (excluding the first column and last column \"status\")\n",
    "        # updated_row = extracted_values + [row[-1]]  # Directly use extracted values since it already includes the URL and status\n",
    "        updated_row = extracted_values # Directly use extracted values since it already includes the URL and status\n",
    "        return updated_row\n",
    "    else:\n",
    "        print(f\"Skipping row with URL: {url} because feature extraction returned None.\")\n",
    "        return None\n",
    "\n",
    "def process_csv(input_file, output_file, num_processes=4):\n",
    "    # Open the input CSV file for reading\n",
    "    with open(input_file, 'r', newline='') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        rows = list(reader)  # Read all rows from the CSV\n",
    "\n",
    "        # Determine the header and data rows\n",
    "        header = rows[0]  # Assuming the first row is header\n",
    "        data_rows = rows[139:140]  # Following rows are data\n",
    "\n",
    "    # Create a pool of worker processes\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        # Process each row in parallel\n",
    "        results = pool.map(process_url, data_rows)\n",
    "\n",
    "    # Filter out None results\n",
    "    results = [row for row in results if row is not None]\n",
    "\n",
    "    # Open the output CSV file for appending\n",
    "    with open(output_file, 'a', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerows(results)\n",
    "        print(f\"CSV file '{output_file}' has been updated.\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_file = '/home/sahil/Desktop/phishing_detection/scripts/modified_file.csv'  # Replace with your input CSV file path\n",
    "    output_csv_file = '/home/sahil/Desktop/phishing_detection/top_200_extracted.csv'  # Replace with your desired output CSV file path\n",
    "    process_csv(input_csv_file, output_csv_file, num_processes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To handle socket(whois ) error. Retry value:3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row with URL: http://www.goepsa.com/Old%20Files/yahoo/IDCHECK.html because feature extraction returned None.\n",
      "Skipping row with URL: http://www.noithatchauau.vn/1./db/box/ because feature extraction returned None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 19:03:51,682 - whois.whois - ERROR - Error trying to connect to socket: closing socket - timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing URL: http://interimmanagement.uk.com/wp-includes/js/chronopost.fr/suivi/accueil/infos/ifram2.php?Code_Smserror_tapnovcodeSms on attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Skipping row with URL: http://sniz.porn because feature extraction returned None.\n",
      "Skipping row with URL: http://enerclim.com/cs/ because feature extraction returned None.\n",
      "Skipping row with URL: https://www.skecherstanio.pl/ because feature extraction returned None.\n",
      "Skipping row with URL: http://support-appleld.com.secureupdate.duilawyeryork.com/ap/78816aad8eccc21 because feature extraction returned None.\n",
      "Skipping row with URL: http://98.126.214.77/ap/signin?openid.pape.max_auth_age=0&amp;openid.return_to=https://www.amazon.co.jp/?ref_=nav_em_hd_re_signin&amp;openid.identity=http://specs.openid.net/auth/2.0/identifier_select&amp;openid.assoc_handle=jpflex&amp;openid.mode=checkid_setup&amp;key=a@b.c&amp;openid.claimed_id=http://specs.openid.net/auth/2.0/identifier_select&amp;openid.ns=http://specs.openid.net/auth/2.0&amp;&amp;ref_=nav_em_hd_clc_signin because feature extraction returned None.\n",
      "Skipping row with URL: http://hidayat.uz/928375602332311ochttp3A2F2F2Fws2FISAPIdllFM2MContact26item3B14276952031324requestePPd3DwolandgioielliPPP26qidQ-jlc@amahousse.com.html because feature extraction returned None.\n",
      "Skipping row with URL: http://www.ilovefreesoftware.com/16/featured/5-best-free-network-packet-sniffer.html because feature extraction returned None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 19:06:21,181 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno -2] Name or service not known\n",
      "2024-06-21 19:06:21,757 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno -2] Name or service not known\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row with URL: http://union-companies.000webhostapp.com/ because feature extraction returned None.\n",
      "Skipping row with URL: https://magalu-crediarioluiza.com/Produto_20203/produto.php?sku=1962067 because feature extraction returned None.\n",
      "Skipping row with URL: http://poligrafiapias.com/Secured-adobe/2b078be1175659f354b82191320e7c51/ because feature extraction returned None.\n",
      "Skipping row with URL: http://pay-pal-deutschland.zxc301.com/029188 because feature extraction returned None.\n",
      "Skipping row with URL: http://www.myappwiz.com/home/redirect?targetUrl=https://livraisonspeed01.cloudaccess.host/wp-includes/DHMF0JCT17E1 because feature extraction returned None.\n",
      "Skipping row with URL: https://support-appleld.com.secureupdate.duilawyeryork.com/ap/c05f378d59c6ba8?cmd=_update&dispatch=c05f378d59c6ba805&locale=_US because feature extraction returned None.\n",
      "Skipping row with URL: http://sma.naturealtree.com/mot?jk=Z4NwlHBmcGKEmLqxy5qmnnx0YpGGjIx0fGBkaXy0j2lj/linda.todd7@nhs.net because feature extraction returned None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 19:08:14,192 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno -2] Name or service not known\n",
      "2024-06-21 19:08:14,767 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno -2] Name or service not known\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row with URL: https://amazon.co.jp.access.usid-9452.mixh.jp/ because feature extraction returned None.\n",
      "Skipping row with URL: http://littlee.com.au/alibaba/login.alibaba.com.php because feature extraction returned None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 19:09:48,053 - whois.whois - ERROR - Error trying to connect to socket: closing socket - timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row with URL: http://www.routeralley.com/guides/nat.pdf because feature extraction returned None.\n",
      "Skipping row with URL: http://transcript.login.2016.getwetusa.com/ because feature extraction returned None.\n",
      "Skipping row with URL: http://maanprofessionals.nl//index.html because feature extraction returned None.\n",
      "Skipping row with URL: http://www.sportshub.com.sg/Directions/Documents/Sports-Hub-Traffic-Brochure.pdf because feature extraction returned None.\n",
      "Skipping row with URL: http://www.m9e4.com/labanquepostale.fr/service-mobile-pass/ac451/ because feature extraction returned None.\n",
      "Skipping row with URL: https://3u8kstpg-97iuu3sdu.vercel.app/ because feature extraction returned None.\n",
      "Skipping row with URL: http://www.instohelp.com/assets_flight/libs/bootstrap/cmcst-wl.html because feature extraction returned None.\n",
      "Skipping row with URL: https://www.workiva.com/ because feature extraction returned None.\n",
      "Skipping row with URL: https://www.jobchannel.ch/ because feature extraction returned None.\n",
      "Skipping row with URL: http://www.neudorf-ole.irv.cc/4B374AA12276932A9FA724E6142E9BD1 because feature extraction returned None.\n",
      "Skipping row with URL: https://safirbetgiristikla.blogspot.com/ because feature extraction returned None.\n",
      "Skipping row with URL: https://www.slant.co/topics/2404/~file-managers-for-windows because feature extraction returned None.\n",
      "Skipping row with URL: http://cartetitolare-it.www1.biz/portaleTitolares15/ because feature extraction returned None.\n",
      "Skipping row with URL: http://judithleoni.com/telecom-de/index.php because feature extraction returned None.\n",
      "Skipping row with URL: http://creditiperhabbogratissicuro100.blogspot.it/2011/02/habbo-crediti-gratis-sicuro-100.html because feature extraction returned None.\n",
      "Skipping row with URL: http://www.apkhere.com/app/com.smartprojects.ramoptimization because feature extraction returned None.\n",
      "Skipping row with URL: http://icloud.com.find.support-lphone.co/ because feature extraction returned None.\n",
      "Skipping row with URL: http://m.hf695.com/ because feature extraction returned None.\n",
      "Skipping row with URL: http://cns-international2.com/r.htm because feature extraction returned None.\n",
      "Skipping row with URL: http://northstaradvisorygroup.com/att/sbc/sbc/sbcglobal.net.htm because feature extraction returned None.\n",
      "Skipping row with URL: http://secure-login-portal-outlook.el.r.appspot.com/c:/users/user/downloads because feature extraction returned None.\n",
      "Skipping row with URL: https://viatraniver1972.blogspot.be because feature extraction returned None.\n",
      "Skipping row with URL: https://norecipes.com/japanese-curry-scratch because feature extraction returned None.\n",
      "Skipping row with URL: http://doc.google.share.pressurecookerindia.com/clients/?7777772e646f632e676f6f676c652e73686172652e7072657373757265636f6f6b6572696e6469612e636f6d.php= because feature extraction returned None.\n",
      "Skipping row with URL: http://www.neutralsources.com/-/re.html because feature extraction returned None.\n",
      "Skipping row with URL: http://www.scancity.am/tmp/ibxolb/ibxolb/login/index-html/login/ because feature extraction returned None.\n",
      "Skipping row with URL: http://kprealtors.com/ve/ because feature extraction returned None.\n",
      "Skipping row with URL: http://www.abbreviations.com/term/16642 because feature extraction returned None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m input_csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/sahil/Desktop/phishing_detection/dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your input CSV file path\u001b[39;00m\n\u001b[1;32m     55\u001b[0m output_csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/sahil/Desktop/phishing_detection/data3.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your desired output CSV file path\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mprocess_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_csv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m, in \u001b[0;36mprocess_csv\u001b[0;34m(input_file, output_file, num_processes)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Create a pool of worker processes\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39mnum_processes) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Process each row in parallel\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_rows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Filter out None results\u001b[39;00m\n\u001b[1;32m     44\u001b[0m filtered_results \u001b[38;5;241m=\u001b[39m [row \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "File \u001b[0;32m~/anaconda3/envs/phishing/lib/python3.12/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/phishing/lib/python3.12/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/phishing/lib/python3.12/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/phishing/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/envs/phishing/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "def process_url(row):\n",
    "    url = row[0]  # Assuming URL is in the first column\n",
    "    retries = 3  # Number of retries\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            extracted_values = fe.extract_features(url)\n",
    "            if extracted_values is not None:\n",
    "                # Ensure extracted_values is a list or tuple\n",
    "                if not isinstance(extracted_values, (list, tuple)):\n",
    "                    print(f\"Skipping row with URL: {url} - Extracted values not a list/tuple.\")\n",
    "                    return None  # Return None to skip this row\n",
    "\n",
    "                # Update the row with extracted values (excluding the first column and last column \"status\")\n",
    "                updated_row = extracted_values + [row[-1]]  # Directly use extracted values since it already includes the URL and status\n",
    "                return updated_row\n",
    "            else:\n",
    "                print(f\"Skipping row with URL: {url} because feature extraction returned None.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL: {url} on attempt {attempt+1}: {e}\")\n",
    "            time.sleep(1)  # Wait for a second before retrying\n",
    "    return None\n",
    "\n",
    "def process_csv(input_file, output_file, num_processes=4):\n",
    "    # Open the input CSV file for reading\n",
    "    with open(input_file, 'r', newline='') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        rows = list(reader)  # Read all rows from the CSV\n",
    "\n",
    "        # Determine the header and data rows\n",
    "        header = rows[0]  # Assuming the first row is header\n",
    "        data_rows = rows[81:200]  # Following rows are data\n",
    "\n",
    "    # Create a pool of worker processes\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        # Process each row in parallel\n",
    "        results = pool.map(process_url, data_rows)\n",
    "\n",
    "    # Filter out None results\n",
    "    filtered_results = [row for row in results if row is not None]\n",
    "\n",
    "    # Open the output CSV file for appending\n",
    "    with open(output_file, 'a', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerows(filtered_results)\n",
    "        print(f\"CSV file '{output_file}' has been updated.\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_file = '/home/sahil/Desktop/phishing_detection/dataset.csv'  # Replace with your input CSV file path\n",
    "    output_csv_file = '/home/sahil/Desktop/phishing_detection/data3.csv'  # Replace with your desired output CSV file path\n",
    "    process_csv(input_csv_file, output_csv_file, num_processes=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phishing",
   "language": "python",
   "name": "phishing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
